import scipy
import numpy as np
import pandas as pd
import sklearn as sk
import sklearn.ensemble
from sklearn import *
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pefile, os, sys, csv
import datetime as dt
from IPython.display import display
from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix


def create_model(dataset):
    """
    :param: dataset: The dataset used for the creation of the Gradient Boosting ML model.

    Create_model() will create a Gradient Boost ML model in order to classify binaries supplied to
    toolkit.py.
    """

    data = pd.read_csv(dataset)
    data.fillna(0)
    columns = data.columns.values.tolist()
    n_records = len(data)
    n_non_malware = len(data['class'] == 0)
    n_malware = len(data['class'] != 0)
    percent_malware = n_malware / n_records * 100
    target = data['class']
    features_raw = data.drop('class', axis=1)
    scaler = MinMaxScaler()
    features_raw_columns = features_raw.columns
    features_raw = pd.DataFrame(scaler.fit_transform(features_raw), columns=features_raw_columns)
    # One-hot encode the 'CLAMP' data using pandas.get_dummies()
    features_final = pd.get_dummies(features_raw)

    # Print the number of features after one-hot encoding
    encoded = list(features_final.columns)

    # This cleans the data and removes any nan values
    features_final = features_final.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)

    # Split the 'features' and 'Target' data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(features_final,
                                                        target,
                                                        test_size=0.2,
                                                        )

    gradient = sklearn.ensemble.GradientBoostingClassifier(n_estimators=50, learning_rate=0.2, max_depth=3,
                                                           )
    gradient.fit(X_train, y_train)
    y_pred = gradient.predict(X_test)
    print('Accuracy of Gradient boosting classifier on test set: {:.2f}'.format(gradient.score(X_test, y_test)))
    multi_conf_mat = multilabel_confusion_matrix(y_test, y_pred, labels=[0, 1, 2, 3])
    #print(multi_conf_mat)
    print(metrics.classification_report(y_test, y_pred))
    return gradient


def predict_classification(model, numerical_list):
    class_dict = {0: 'Non-Malware', 1: 'Ransomware', 2: 'Trojan', 3: 'Adware'}
    new_input = np.array(numerical_list).reshape(1, -1)
    new_output = model.predict(new_input)
    percent_pred = model.predict_proba(new_input)
    index = 0
    array = percent_pred[0]
    print("CLASSIFICATION PROBABILITIES:")
    for index in range(0, 4):
        print(class_dict[index] + ": " + str(round(array[index]*100)) + "%")
        index += 1
    new_output = class_dict[new_output[0]]
    return new_output


def create_dataset():
    """
    This function will create a .csv file to be used as a dataset for creating the ML model.
    Supply a directory into the os.walk() function in order to use your own directory of binaries,
    otherwise, a dataset has already been created and supplied in ../storedData
    """
    for root, dirs, files in os.walk('../goodSamples/goodSamples'):
        for file in files:
            # print(root + '/' + file)
            fullpath = root + '/' + file
            extract_properties(fullpath)


def append_to_csv(numerical_list, filename='../storedData/binaryData.csv'):
    with open(filename, 'a+') as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(numerical_list)


def extract_properties(file):
    """
    :param file: the binary file to extract properties from

    Uses the pefile module to extract 57 features of a PE file in order to be used
    for classification purposes with the Gradient Boosting ML model. This takes elements from the
    DOS_HEADER, FILE_HEADER, and OPTIONAL_HEADER
    """
    class_dict = {'Ransomware': 1, 'Trojan': 2, 'Adware': 3}
    try:
        binary = pefile.PE(file, fast_load=True)
    except pefile.PEFormatError:
        return
    numerical_list = []
    # total = list(binary.print_info())
    dos_header_dict = (binary.DOS_HEADER.dump_dict())
    del dos_header_dict['Structure']
    for key in dos_header_dict.keys():
        value = dos_header_dict[key]['Value']
        if type(value) != int:
            value = 0
        numerical_list.append(value)
    numerical_list.append(binary.NT_HEADERS.Signature)
    file_header_dict = binary.FILE_HEADER.dump_dict()
    del file_header_dict['Structure']
    for key in file_header_dict.keys():
        if key == 'TimeDateStamp':
            value = (file_header_dict[key]['Value'].split(' '))
            test = dt.datetime.utcfromtimestamp(float(int(value[0], 16)))
            year = test.year
            numerical_list.append(year)
        else:
            value = file_header_dict[key]['Value']
            if type(value) != int:
                value = 0
            numerical_list.append(value)
    optional_header_dict = binary.OPTIONAL_HEADER.dump_dict()
    del optional_header_dict['Structure']
    if 'BaseOfData' not in optional_header_dict.keys():
        optional_header_dict['BaseOfData'] = {'Value': 0}
    for key in optional_header_dict.keys():
        value = optional_header_dict[key]['Value']
        if type(value) != int:
            value = 0
        numerical_list.append(value)
    # BLOCKED OUT CODE USED ONLY FOR CREATION OF DATASET;
    # file_type = file.split('-')[0]
    # file_type = file_type.split('/')[-1]
    # class_type = class_dict[file_type]
    # class_type = 0
    # numerical_list.append(class_type)
    # BLOCKED OUT CODE USED ONLY FOR CREATION OF DATASET;
    # append_to_csv(numerical_list)
    return numerical_list


if __name__ == '__main__':
    print("Used as a module with toolkit.py")
    exit(1)
